import time
import random
import torch
from text_edu import TextAgent, SimpleRussianTokenizer

# --- 1. –†–ê–°–®–ò–†–Ø–ï–ú –°–õ–û–í–ê–†–¨ (–ù—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –∫–ª–∞—Å—Å Tokenizer) ---
# –ß—Ç–æ–±—ã –∞–≥–µ–Ω—Ç –ø–æ–Ω–∏–º–∞–ª –Ω–æ–≤—ã–µ —Å–ª–æ–≤–∞, –º—ã –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–∏–º –∏—Ö "–Ω–∞ –ª–µ—Ç—É"
# –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º, –Ω–æ –∑–¥–µ—Å—å —Å–¥–µ–ª–∞–µ–º –ø–∞—Ç—á.

def expand_vocab(agent):
    new_words = [
        # Nature & Elements
        "–≤–æ–¥–∞", "–æ–≥–æ–Ω—å", "–∑–µ–º–ª—è", "–≤–æ–∑–¥—É—Ö", "—Ç–µ—á–µ—Ç", "–≥–æ—Ä–∏—Ç", "–¥—ã—à–∏—Ç", 
        "—Ä–µ–∫–∞", "–≥–æ—Ä–∞", "–ª–µ—Å", "–ø–æ–ª–µ", "–Ω–µ–±–æ", "–≤–µ—Ç–µ—Ä", "–¥–æ–∂–¥—å", "—Å–Ω–µ–≥",
        "—Å–æ–ª–Ω—Ü–µ", "–ª—É–Ω–∞", "–Ω–æ—á—å", "–¥–µ–Ω—å", "—É—Ç—Ä–æ", "–≤–µ—á–µ—Ä", "–∫–æ—Å–º–æ—Å", "–∑–≤–µ–∑–¥—ã",
        
        # Qualities
        "—Ö–æ–ª–æ–¥–Ω–æ", "–∂–∞—Ä–∫–æ", "–ø—Ä–∏—è—Ç–Ω–æ", "–æ–ø–∞—Å–Ω–æ", "—Ç–∏—à–∏–Ω–∞", "–≥—Ä–æ–º–∫–æ", "—Ö–æ—Ä–æ—à–æ", "–ø–ª–æ—Ö–æ",
        "–∑–µ–ª–µ–Ω—ã–π", "–±–µ–ª—ã–π", "—á–µ—Ä–Ω—ã–π", "–±—ã—Å—Ç—Ä–æ", "–º–µ–¥–ª–µ–Ω–Ω–æ", "–¥–∞–ª–µ–∫–æ", "–±–ª–∏–∑–∫–æ", "–Ω–æ–≤—ã–π", "—Å—Ç–∞—Ä—ã–π",
        
        # Actions (Verbs)
        "–∏–¥—Ç–∏", "—Å—Ç–æ—è—Ç—å", "—Å–º–æ—Ç—Ä–µ—Ç—å", "—Å–ª—É—à–∞—Ç—å", "–±–µ–∂–∞—Ç—å", "–ª–µ—Ç–µ—Ç—å", "—Å–ø–∞—Ç—å", 
        "–≥–æ–≤–æ—Ä–∏—Ç—å", "–º–æ–ª—á–∞—Ç—å", "–±—Ä–∞—Ç—å", "–¥–∞–≤–∞—Ç—å", "—Å–æ–∑–¥–∞–≤–∞—Ç—å", "—Ä–∞–∑—Ä—É—à–∞—Ç—å", 
        "–¥—É–º–∞—Ç—å", "–∑–Ω–∞—Ç—å", "–≤–µ—Ä–∏—Ç—å", "—Å–ª—ã—à–∞—Ç—å", "–ø–æ–º–Ω–∏—Ç—å", "–∑–∞–±—ã—Ç—å", "–Ω–∞–π—Ç–∏", 
        "–ø–æ—Ç–µ—Ä—è—Ç—å", "–Ω–∞—á–∞—Ç—å", "–∫–æ–Ω—á–∏—Ç—å", "–∂–∏—Ç—å", "—É–º–µ—Ä–µ—Ç—å", "–ª—é–±–∏—Ç—å", "–Ω–µ–Ω–∞–≤–∏–¥–µ—Ç—å", 
        "—Å—Ç—Ä–æ–∏—Ç—å", "–ª–æ–º–∞—Ç—å",
        
        # Abstract & Social
        "–¥—Ä—É–≥", "–≤—Ä–∞–≥", "–∏—Å—Ç–∏–Ω–∞", "–ª–æ–∂—å", "—Å–∏–ª–∞", "—Å–ª–∞–±–æ—Å—Ç—å", "–∫—Ä–∞—Å–æ—Ç–∞", 
        "—Ö–∞–æ—Å", "–ø–æ—Ä—è–¥–æ–∫", "—Ü–µ–ª—å", "–ø—É—Ç—å", "—Å–≤–æ–±–æ–¥–∞"
    ]
    agent.tokenizer.add_tokens(new_words)
    
    # –ù–∞–º –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, —Ç–∞–∫ –∫–∞–∫ —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –≤—ã—Ä–æ—Å
    old_embeddings = agent.interface.embeddings.weight.data
    old_vocab_size = old_embeddings.shape[0]
    new_vocab_size = agent.tokenizer.vocab_size
    
    if new_vocab_size > old_vocab_size:
        print(f"–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è: {old_vocab_size} -> {new_vocab_size}")
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        new_embeddings = torch.nn.Embedding(new_vocab_size, agent.interface.emb_dim).to(agent.interface.device)
        # –ö–æ–ø–∏—Ä—É–µ–º —Å—Ç–∞—Ä—ã–µ –∑–Ω–∞–Ω–∏—è
        new_embeddings.weight.data[:old_vocab_size] = old_embeddings
        # –ó–∞–º–µ–Ω—è–µ–º —Å–ª–æ–π –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ
        agent.interface.embeddings = new_embeddings

# --- 2. –ö–û–†–ü–£–° –¢–ï–ö–°–¢–û–í (–î–µ—Ç—Å–∫–∏–π –±—É–∫–≤–∞—Ä—å) ---
corpus = [
    "–Ø –≤–∏–∂—É —Å–≤–µ—Ç", "–°–≤–µ—Ç –µ—Å—Ç—å –¥–µ–Ω—å", "–¢—å–º–∞ –µ—Å—Ç—å –Ω–æ—á—å",
    "–û–≥–æ–Ω—å —ç—Ç–æ –∂–∞—Ä–∫–æ", "–û–≥–æ–Ω—å —ç—Ç–æ –±–æ–ª—å–Ω–æ", "–û–≥–æ–Ω—å —ç—Ç–æ –æ–ø–∞—Å–Ω–æ",
    "–í–æ–¥–∞ —ç—Ç–æ –∂–∏–∑–Ω—å", "–í–æ–¥–∞ —Ç–µ—á–µ—Ç", "–Ø –ø—å—é –≤–æ–¥—É",
    "–°–æ–ª–Ω—Ü–µ –¥–∞–µ—Ç —Å–≤–µ—Ç", "–õ—É–Ω–∞ –¥–∞–µ—Ç —Å–æ–Ω", "–Ø —Ö–æ—á—É —Å–ø–∞—Ç—å",
    "–ë–æ–ª—å —ç—Ç–æ –ø–ª–æ—Ö–æ", "–†–∞–¥–æ—Å—Ç—å —ç—Ç–æ —Ö–æ—Ä–æ—à–æ", "–õ—é–±–æ–≤—å –µ—Å—Ç—å —Ä–∞–¥–æ—Å—Ç—å",
    "–Ø –∏–¥—É –∏—Å–∫–∞—Ç—å", "–ú—ã –≤–∏–¥–∏–º –º–∏—Ä", "–ú–∏—Ä –±–æ–ª—å—à–æ–π",
    "–í—Ä–µ–º—è —Ç–µ—á–µ—Ç", "–ñ–∏–∑–Ω—å –µ—Å—Ç—å –≤—Ä–µ–º—è", "–Ø –º—ã—Å–ª—é —Å–º—ã—Å–ª",
    "–¢—ã –º–æ–π –¥—Ä—É–≥", "–í—Ä–∞–≥ —ç—Ç–æ –æ–ø–∞—Å–Ω–æ", "–î—Ä—É–≥ —ç—Ç–æ —Ö–æ—Ä–æ—à–æ"
]

def run_lifelong_learning():
    agent = TextAgent()
    
    # –ü–´–¢–ê–ï–ú–°–Ø –ó–ê–ì–†–£–ó–ò–¢–¨ –ü–†–û–®–õ–£–Æ –ñ–ò–ó–ù–¨
    agent.load_brain("agent_soul.pt")
    
    # –ï—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤—ã–π –∞–≥–µ–Ω—Ç, —Ä–∞—Å—à–∏—Ä—è–µ–º —Å–ª–æ–≤–∞—Ä—å. –ï—Å–ª–∏ —Å—Ç–∞—Ä—ã–π - —Å–ª–æ–≤–∞—Ä—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω.
    if agent.tokenizer.vocab_size < 120:  
        expand_vocab(agent) 
    
    print("\n=== –ó–ê–ü–£–°–ö –ù–ï–ü–†–ï–†–´–í–ù–û–ì–û –ñ–ò–ó–ù–ï–ù–ù–û–ì–û –¶–ò–ö–õ–ê (–° –ü–ê–ú–Ø–¢–¨–Æ) ===")
    
    step = 0
    try:
        while True:
            step += 1
            print(f"\n--- [–¶–∏–∫–ª {step}] ---")
            
            # 1. –°–ª—É—à–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é —Ñ—Ä–∞–∑—É (–û–±—É—á–µ–Ω–∏–µ)
            phrase = random.choice(corpus)
            epochs = random.choice([1, 1, 2])
            
            success = agent.listen_and_learn(phrase, epochs=epochs)
            
            if not success:
                print("ü§í –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ...")
                silence = agent.interface.token_to_sensor_input(agent.tokenizer.token2id["<SILENCE>"])
                agent.brain.body.sensory_input = silence
                for _ in range(3): 
                    for _ in range(100): agent.brain.step(agent.dt)
                    agent.force_calm(severity=0.2)
                continue

            # 2. –ê–∫—Ç–∏–≤–Ω—ã–π –≤—ã–≤–æ–¥
            if step % 5 == 0:
                trigger_id = random.randint(5, agent.tokenizer.vocab_size - 1)
                trigger_word = agent.tokenizer.id2token[trigger_id]
                print(f"\n>>> –ê–≥–µ–Ω—Ç —Ö–æ—á–µ—Ç –≤—ã—Å–∫–∞–∑–∞—Ç—å—Å—è –Ω–∞ —Ç–µ–º—É '{trigger_word}'...")
                response = agent.generate_text(trigger_word, max_length=6, temperature=0.6)
                print(f"AGENT: {response}")

            # 3. –°–û–ù + –°–û–•–†–ê–ù–ï–ù–ò–ï
            if step % 50 == 0: # –ö–∞–∂–¥—ã–µ 50 —Ü–∏–∫–ª–æ–≤ - –∞–≤—Ç–æ—Å–µ–π–≤
                print("\nüíæ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ...")
                agent.save_brain("agent_soul.pt")
                
            if step % 10 == 0:
                print("\nüí§ –ì–ª—É–±–æ–∫–∏–π –æ—Ç–¥—ã—Ö...")
                agent.force_calm(severity=0.4) 
                time.sleep(1)

    except KeyboardInterrupt:
        print("\n\nüõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞...")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ–¥ –≤—ã—Ö–æ–¥–æ–º
        agent.save_brain("agent_soul.pt")
        print(f"–ê–≥–µ–Ω—Ç –ø—Ä–æ–∂–∏–ª {step} —Ü–∏–∫–ª–æ–≤ –∏ –±—ã–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω.")

if __name__ == "__main__":
    run_lifelong_learning()
