import torch
import numpy as np
import time
from typing import List, Dict, Tuple
from dataclasses import dataclass

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —è–¥—Ä–æ —Å–∏–º—É–ª—è—Ç–æ—Ä–∞
# –í–∞–∂–Ω–æ: Neuralbiocore_U_for_GPU.py –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ
try:
    from Neuralbiocore_U_for_GPU import ConsciousnessSimulator, PhysicsConfig, BioChemistry
except ImportError:
    raise ImportError("–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª Neuralbiocore_U_for_GPU.py. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ä—è–¥–æ–º.")

# ==========================================
# 1. –¢–û–ö–ï–ù–ò–ó–ê–¢–û–† (–†—É—Å—Å–∫–∏–π —è–∑—ã–∫)
# ==========================================

class SimpleRussianTokenizer:
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞ –≤ –∏–Ω–¥–µ–∫—Å—ã –∏ –æ–±—Ä–∞—Ç–Ω–æ.
    """
    def __init__(self):
        self.token2id: Dict[str, int] = {}
        self.id2token: Dict[int, str] = {}
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã:
        # PAD - –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ, UNK - –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ, BOS - –Ω–∞—á–∞–ª–æ, EOS - –∫–æ–Ω–µ—Ü, SILENCE - —Ç–∏—à–∏–Ω–∞
        self.specials = ["<PAD>", "<UNK>", "<BOS>", "<EOS>", "<SILENCE>"]
        for i, token in enumerate(self.specials):
            self.token2id[token] = i
            self.id2token[i] = token
            
        self.vocab_size = len(self.specials)
        
        # –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å (–§–∏–ª–æ—Å–æ—Ñ—Å–∫–æ-–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –Ω–∞–±–æ—Ä)
        initial_vocab = [
            "—è", "—Ç—ã", "–º—ã", "–æ–Ω", "–º–∏—Ä", "—Å–æ–∑–Ω–∞–Ω–∏–µ", "–±–æ–ª—å", "—Ä–∞–¥–æ—Å—Ç—å", 
            "–≤–∏–∂—É", "—á—É–≤—Å—Ç–≤—É—é", "–¥—É–º–∞—é", "–µ—Å—Ç—å", "–Ω–µ—Ç", "–±—ã—Ç—å", "—Å–≤–µ—Ç", 
            "—Ç—å–º–∞", "–≤—Ä–µ–º—è", "–ø–æ–∏—Å–∫", "—Å–º—ã—Å–ª", "–∂–∏–∑–Ω—å", "—Å–æ–Ω", "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ",
            "–∫—Ä–∞—Å–Ω—ã–π", "—Å–∏–Ω–∏–π", "—Ñ–æ—Ä–º–∞", "–æ–±—ä–µ–∫—Ç", "—Å—É–±—ä–µ–∫—Ç", "–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å",
            "–ª—é–±–æ–≤—å", "—Å—Ç—Ä–∞—Ö", "–ø–æ–Ω–∏–º–∞—é", "–Ω–µ", "–∏", "–Ω–æ", "–≥–¥–µ", "–∑–∞—á–µ–º",
            "—á–µ–ª–æ–≤–µ–∫", "—Ä–∞–∑—É–º", "—Å–∏–≥–Ω–∞–ª", "–≤—Å–µ–ª–µ–Ω–Ω–∞—è", "–ø—É—Å—Ç–æ—Ç–∞",
            "—á–∏—Å–ª–æ", "–º–∞—à–∏–Ω–∞", "–∫–æ–¥", "—Å–∏—Å—Ç–µ–º–∞", "—ç–Ω–µ—Ä–≥–∏—è"
        ]
        self.add_tokens(initial_vocab)

    def add_tokens(self, tokens: List[str]):
        for token in tokens:
            token = token.lower().strip()
            if token not in self.token2id:
                self.token2id[token] = self.vocab_size
                self.id2token[self.vocab_size] = token
                self.vocab_size += 1

    def encode(self, text: str) -> List[int]:
        # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–æ–±–µ–ª–∞–º
        tokens = text.lower().replace('.', ' .').replace(',', ' ,').replace('?', ' ?').split()
        ids = [self.token2id.get(t, self.token2id["<UNK>"]) for t in tokens]
        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ BOS (Begin) –∏ EOS (End)
        return [self.token2id["<BOS>"]] + ids + [self.token2id["<EOS>"]]

    def decode(self, ids: List[int]) -> str:
        tokens = []
        for i in ids:
            if i in self.id2token:
                t = self.id2token[i]
                if t not in self.specials:
                    tokens.append(t)
        return " ".join(tokens)

# ==========================================
# 2. –ò–ù–¢–ï–†–§–ï–ô–° ("–£—à–∏ –∏ –†–æ—Ç")
# ==========================================

class NeuroLinguisticInterface:
    """
    –°–≤—è–∑—ã–≤–∞–µ—Ç –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é.
    """
    def __init__(self, simulator: ConsciousnessSimulator, tokenizer: SimpleRussianTokenizer, embedding_dim: int = 64):
        self.sim = simulator
        self.tokenizer = tokenizer
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (CPU/GPU) –ø–æ —Ç–æ–º—É, –≥–¥–µ –∂–∏–≤–µ—Ç –ø–µ—Ä–≤—ã–π —Å–ª–æ–π –º–æ–∑–≥–∞
        self.device = simulator.hierarchy.levels[0].layer.V.device
        
        # –†–∞–∑–º–µ—Ä —Å–µ–Ω—Å–æ—Ä–Ω–æ–≥–æ —Å–ª–æ—è V1 (–æ–±—ã—á–Ω–æ 1000 –Ω–µ–π—Ä–æ–Ω–æ–≤)
        self.sensor_dim = simulator.hierarchy.levels[0].layer.N 
        self.emb_dim = embedding_dim
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ (–°–ª–æ–≤–∞—Ä—å –≤–µ–∫—Ç–æ—Ä–æ–≤)
        self.embeddings = torch.nn.Embedding(tokenizer.vocab_size, embedding_dim).to(self.device)
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ V1 (W_sensory)
        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç 0.015 –ø–æ–¥–æ–±—Ä–∞–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –Ω–µ–π—Ä–æ–Ω—ã
        self.W_sensor_projection = torch.randn(self.sensor_dim, embedding_dim).to(self.device) * 0.015
        
    def token_to_sensor_input(self, token_id: int) -> torch.Tensor:
        """
        –£–®–ò: –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç ID —Ç–æ–∫–µ–Ω–∞ –≤ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π —Ç–æ–∫ –¥–ª—è —Å–µ–Ω—Å–æ—Ä–æ–≤.
        """
        with torch.no_grad():
            token_tensor = torch.tensor([token_id], device=self.device)
            emb = self.embeddings(token_tensor).squeeze(0)
            
            # Input = W * Embedding
            sensor_signal = torch.mv(self.W_sensor_projection, emb)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å (—à—É–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è)
            sensor_signal += torch.randn_like(sensor_signal) * 0.005
            
            return sensor_signal

    def neural_state_to_logits(self, neural_prediction_mu: torch.Tensor) -> torch.Tensor:
        """
        –†–û–¢: –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–∑–≥–∞ (V1 mu) –æ–±—Ä–∞—Ç–Ω–æ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–æ–µ–∫—Ü–∏–∏ (Inverse Model).
        """
        # Emb_pred = W.T * Neural_State
        # [FIX] Cast to match projection matrix dtype (float32 vs float64)
        neural_prediction_mu = neural_prediction_mu.to(self.W_sensor_projection.dtype)
        
        emb_pred = torch.mv(self.W_sensor_projection.T, neural_prediction_mu)
        
        # Logits = Embeddings * Emb_pred (–ø–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ —Å–ª–æ–≤–∞)
        logits = torch.mv(self.embeddings.weight, emb_pred)
        return logits

# ==========================================
# 3. –ê–ì–ï–ù–¢ (–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏–µ–º)
# ==========================================

class TextAgent:
    def __init__(self):
        self.tokenizer = SimpleRussianTokenizer()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–∑–≥ —Å —Ç–æ–ø–æ–ª–æ–≥–∏–µ–π Small-World (–ª—É—á—à–µ –¥–ª—è –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π)
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏...")
        self.brain = ConsciousnessSimulator(use_small_world=True)
        
        self.interface = NeuroLinguisticInterface(self.brain, self.tokenizer)
        
        # –í—Ä–µ–º—è –ø—Ä–µ–¥—ä—è–≤–ª–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
        # 50 –º—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, –Ω–æ –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–µ—Ç
        self.token_duration = 0.050 
        self.dt = self.brain.phys_cfg.dt
        
        # –ü–æ—Ä–æ–≥ –ø–∞–Ω–∏–∫–∏: –µ—Å–ª–∏ –°–≤–æ–±–æ–¥–Ω–∞—è –≠–Ω–µ—Ä–≥–∏—è –≤—ã—à–µ, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        self.panic_threshold = 4_500_000.0 

        # –°–ø–∏—Å–æ–∫ –ø–æ–Ω—è—Ç–∏–π —Å –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–π –æ–∫—Ä–∞—Å–∫–æ–π (Semantic Aversion)
        self.negative_concepts = {
            "–±–æ–ª—å", "—Å—Ç—Ä–∞—Ö", "–ø–ª–æ—Ö–æ", "—Ç—å–º–∞", "–≤—Ä–∞–≥", "–Ω–µ–Ω–∞–≤–∏–¥–µ—Ç—å", 
            "—Å–º–µ—Ä—Ç—å", "—É–∂–∞—Å", "–æ–ø–∞—Å–Ω–æ", "—Ä–∞–∑—Ä—É—à–∞—Ç—å", "–ª–æ–º–∞—Ç—å"
        }
        
        print(f"–ê–≥–µ–Ω—Ç –≥–æ—Ç–æ–≤. Device: {self.interface.device}")

    def force_calm(self, severity: float = 1.0):
        """
        –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –≥–æ–º–µ–æ—Å—Ç–∞–∑ v2.0 (Fix Sparse Update).
        """
        with torch.no_grad():
            # 1. –•–∏–º–∏—è: –°–Ω–∏–∂–∞–µ–º –Ω–µ–π—Ä–æ–º–æ–¥—É–ª—è—Ç–æ—Ä—ã
            self.brain.chemistry.dopamine.mul_(0.5 * (1.0 - severity))
            self.brain.chemistry.norepinephrine.mul_(0.5 * (1.0 - severity))
            
            # 2. –≠–ª–µ–∫—Ç—Ä–∏–∫–∞: –ì–∞—Å–∏–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            for unit in self.brain.hierarchy.levels:
                unit.layer.I_ext.zero_()
                unit.layer.V.mul_(0.1) 
                unit.prediction_error.zero_()
                
                # === –õ–ï–ß–ï–ù–ò–ï –≠–ü–ò–õ–ï–ü–°–ò–ò ===
                if severity > 0.5:
                    # –£—Å–∏–ª–∏–≤–∞–µ–º —Ç–µ—Ä–∞–ø–∏—é: 20% –∑–∞–±—ã–≤–∞–Ω–∏—è –≤–º–µ—Å—Ç–æ 5%
                    decay = 0.8 
                    
                    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–µ—á–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ —Å–∏–Ω–∞–ø—Å–∞
                    def heal_synapse(synapse):
                        if synapse is None: return
                        
                        # 1. –û—Å–ª–∞–±–ª—è–µ–º –≤–µ—Å–∞
                        if hasattr(synapse, 'W_dense'):
                            synapse.W_dense.mul_(decay)
                            
                            # 2. CLAMP: –ñ–µ—Å—Ç–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –≤–µ—Å–∞
                            # –°—Ä–µ–∑–∞–µ–º –≤—Å–µ, —á—Ç–æ –≤—ã—Ä–æ—Å–ª–æ –≤—ã—à–µ 1.5 (–ø—Ä–µ–¥–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å)
                            synapse.W_dense.clamp_(max=1.5)
                            
                            # 3. –í–ê–ñ–ù–û: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º Sparse –º–∞—Ç—Ä–∏—Ü—É, –µ—Å–ª–∏ –æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
                            if hasattr(synapse, 'is_sparse') and synapse.is_sparse:
                                synapse.W_sparse = synapse.W_dense.to_sparse_csr()
                        else:
                            # –ï—Å–ª–∏ —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ dense/sparse —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
                            synapse.W.mul_(decay)
                            synapse.W.clamp_(max=1.5)

                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ –≤—Ö–æ–¥–∞–º (bottom-up) –∏ –æ–∂–∏–¥–∞–Ω–∏—è–º (top-down)
                    heal_synapse(unit.synapse_bottom_up)
                    heal_synapse(unit.synapse_top_down)
            
            # 3. –°–±—Ä–æ—Å GWT
            self.brain.gwt.broadcast_signal.zero_()
            self.brain.gwt.active_coalitions = []
            
            if severity > 0.5:
                print("   [Homeostasis] –í–ï–°–ê –°–ë–†–û–®–ï–ù–´ –ò –°–ò–ù–•–†–û–ù–ò–ó–ò–†–û–í–ê–ù–´ (Sparse Sync).")

    def apply_negative_reinforcement(self, severity: float = 1.0):
        """
        –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –û–¢–†–ò–¶–ê–¢–ï–õ–¨–ù–û–ï –ü–û–î–ö–†–ï–ü–õ–ï–ù–ò–ï (Anti-Hebbian).
        –ù–∞–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–Ω–∞–ø—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –∞–∫—Ç–∏–≤–Ω—ã –≤ –º–æ–º–µ–Ω—Ç –æ—à–∏–±–∫–∏/–±–æ–ª–∏.
        """
        print(f"   ‚ò†Ô∏è NEGATIVE REINFORCEMENT (Severity: {severity:.2f})")
        
        with torch.no_grad():
            # 1. –•–∏–º–∏—è –°–¢–†–ï–°–°–ê
            # –î–æ—Ñ–∞–º–∏–Ω –≤ –Ω–æ–ª—å (–Ω–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã), –ù–æ—Ä–∞–¥—Ä–µ–Ω–∞–ª–∏–Ω –≤ –º–∞–∫—Å–∏–º—É–º (–ø–∞–Ω–∏–∫–∞/–∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–∞)
            self.brain.chemistry.dopamine.mul_(0.0)
            self.brain.chemistry.norepinephrine.fill_(0.8 + 0.2 * severity)
            
            # 2. Anti-Hebbian Learning –¥–ª—è –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π
            for level_idx, unit in enumerate(self.brain.hierarchy.levels):
                
                # –§—É–Ω–∫—Ü–∏—è –Ω–∞–∫–∞–∑–∞–Ω–∏—è —Å–∏–Ω–∞–ø—Å–∞
                def punish_synapse(synapse):
                    if synapse is None: return
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—ã –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (Eligibility Traces)
                    # trace_pre: [N_pre], trace_post: [N_post]
                    # Co-activity ~ OuterProduct(post, pre)
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –¥–æ—Å—Ç–∞—Ç—å —Å–ª–µ–¥—ã. –ï—Å–ª–∏ –∏—Ö –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–µ —Å–ø–∞–π–∫–∏ –∫–∞–∫ proxy (–º–µ–Ω–µ–µ —Ç–æ—á–Ω–æ)
                    # –í Neuralbiocore_U_for_GPU.py —ç—Ç–æ trace_pre –∏ trace_post
                    if not hasattr(synapse, 'trace_pre') or not hasattr(synapse, 'trace_post'):
                        return

                    # –í—ã—á–∏—Å–ª—è–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (–∫—Ç–æ –≤–∏–Ω–æ–≤–∞—Ç?)
                    # [N_post, 1] * [1, N_pre] -> [N_post, N_pre]
                    eligibility = torch.ger(synapse.trace_post, synapse.trace_pre)
                    
                    # –ù–∞–∫–∞–∑—ã–≤–∞–µ–º: W = W - severity * learning_rate * eligibility
                    punishment_strength = 0.5 * severity # –°–∏–ª–∞ –Ω–∞–∫–∞–∑–∞–Ω–∏—è
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ Dense –º–∞—Ç—Ä–∏—Ü–µ
                    if hasattr(synapse, 'W_dense'):
                        # W -= punishment
                        synapse.W_dense.sub_(eligibility * punishment_strength)
                        synapse.W_dense.clamp_(min=0.0) # –ù–µ –¥–∞–µ–º –≤–µ—Å–∞–º —Å—Ç–∞—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏
                        
                        # Sync Sparse
                        if hasattr(synapse, 'is_sparse') and synapse.is_sparse:
                            synapse.W_dense.masked_fill_(~synapse.mask, 0.0) # Mask restore
                            synapse.W_sparse = synapse.W_dense.to_sparse_csr()
                    elif hasattr(synapse, 'W'):
                        synapse.W.sub_(eligibility * punishment_strength)
                        synapse.W.clamp_(min=0.0)

                # –ù–∞–∫–∞–∑—ã–≤–∞–µ–º —Å–≤—è–∑–∏
                punish_synapse(unit.synapse_bottom_up)
                punish_synapse(unit.synapse_top_down)
                
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–µ–π—Ä–æ–Ω–æ–≤ (GABA Flush)
                unit.layer.V.fill_(self.brain.phys_cfg.v_rest)
                unit.layer.is_dead.fill_(False) # –í–æ—Å–∫—Ä–µ—à–∞–µ–º, –µ—Å–ª–∏ —É–º–µ—Ä–ª–∏ –æ—Ç —à–æ–∫–∞
                unit.layer.ATP.fill_(0.5) # –î–∞–µ–º —ç–Ω–µ—Ä–≥–∏—é –Ω–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ

    def listen_and_learn(self, text: str, epochs: int = 1) -> bool:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç False, –µ—Å–ª–∏ —Å–ª—É—á–∏–ª–∞—Å—å –ø–∞–Ω–∏–∫–∞.
        """
        token_ids = self.tokenizer.encode(text)
        steps_per_token = int(self.token_duration / self.dt)
        
        print(f"\n--- –û–±—É—á–µ–Ω–∏–µ —Ñ—Ä–∞–∑–µ: '{text}' ---")
        
        # –õ–µ–≥–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–¥ —Ñ—Ä–∞–∑–æ–π
        self.force_calm(severity=0.5)
        
        for epoch in range(epochs):
            total_free_energy = 0.0
            panic_mode = False
            
            for t, token_id in enumerate(token_ids):
                if panic_mode: break
                
                sensory_signal = self.interface.token_to_sensor_input(token_id)
                self.brain.body.sensory_input = sensory_signal
                
                current_token_fe = 0.0
                for _ in range(steps_per_token):
                    self.brain.step(self.dt)
                    fe = self.brain.hierarchy.get_global_free_energy()
                    current_token_fe += fe
                
                avg_token_fe = current_token_fe / steps_per_token
                total_free_energy += avg_token_fe
                
                # Check for Semantic Aversion (–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è –æ–∫—Ä–∞—Å–∫–∞ —Å–ª–æ–≤–∞)
                token_str = self.tokenizer.id2token.get(token_id, "")
                if token_str in self.negative_concepts:
                    print(f"   ‚ö†Ô∏è Semantic Aversion: '{token_str}' detected.")
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ú–Ø–ì–ö–û–ï –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ (Low Severity)
                    # –ß—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞–≤–º–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Ç—å, –∞ –ª–∏—à—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ
                    self.apply_negative_reinforcement(severity=0.2) 

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–∞
                if avg_token_fe > self.panic_threshold:
                    token_str = self.tokenizer.id2token.get(token_id, "???")
                    print(f"   ! –®–û–ö –Ω–∞ —Å–ª–æ–≤–µ '{token_str}': FE={avg_token_fe:.0f}")
                    panic_mode = True
                    break

            if panic_mode:
                print(f"   !!! –í–ö–õ–Æ–ß–ï–ù–ò–ï –ü–†–û–¢–û–ö–û–õ–ê –ó–ê–©–ò–¢–´.")
                # self.force_calm(severity=1.0) 
                self.apply_negative_reinforcement(severity=1.5) # NEW WAY: Punish active pathways
                return False # <--- –í–µ—Ä–Ω—É–ª–∏ False –ø—Ä–∏ –æ—à–∏–±–∫–µ
            
            # –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –ø–∞–Ω–∏–∫–∞)
            avg_epoch_fe = total_free_energy / len(token_ids)
            print(f"Epoch {epoch+1} | Avg FE: {avg_epoch_fe:.0f} | Phi: {self.brain.gwt.phi_current:.2f}")
            
            # –ü–∞—É–∑–∞
            silence = self.interface.token_to_sensor_input(self.tokenizer.token2id["<SILENCE>"])
            self.brain.body.sensory_input = silence
            for _ in range(30): self.brain.step(self.dt)
            
        return True # <--- –í–ê–ñ–ù–û! –≠—Ç–∞ —Å—Ç—Ä–æ–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ó–î–ï–°–¨, –≤–Ω–µ —Ü–∏–∫–ª–∞ for

    def generate_text(self, prompt: str, max_length: int = 8, temperature: float = 0.8) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ê–∫—Ç–∏–≤–Ω—ã–π –í—ã–≤–æ–¥ (Active Inference).
        """
        print(f"\n--- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (Prompt: '{prompt}') ---")
        
        # –õ–µ–≥–∫–æ–µ —É—Å–ø–æ–∫–æ–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ —Ä–µ—á—å—é
        self.force_calm(severity=0.1)
        
        prompt_ids = self.tokenizer.encode(prompt)
        steps_per_token = int(self.token_duration / self.dt)
        
        # 1. Priming: –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (—Å–ª—É—à–∞–µ–º –ø—Ä–æ–º–ø—Ç)
        for token_id in prompt_ids:
            if token_id == self.tokenizer.token2id["<EOS>"]: continue
            sensory_signal = self.interface.token_to_sensor_input(token_id)
            self.brain.body.sensory_input = sensory_signal
            for _ in range(steps_per_token):
                self.brain.step(self.dt)

        # 2. Generation Loop: –ì–æ–≤–æ—Ä–∏–º
        generated_ids = []
        # –ù–∞—á–∏–Ω–∞–µ–º —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ–≤–∞ –ø—Ä–æ–º–ø—Ç–∞
        current_input_id = prompt_ids[-2] if len(prompt_ids) > 1 else prompt_ids[0]
        
        # –°–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∏–∑–±–µ–≥–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–æ–≤
        history_window = [current_input_id]
        
        for _ in range(max_length):
            # –≠—Ö–æ-—Å–∏–≥–Ω–∞–ª (Self-feedback)
            # –í–º–µ—Å—Ç–æ —Ç–∏—à–∏–Ω—ã –ø–æ–¥–∞–µ–º —Å–ª–∞–±–æ–µ "—ç—Ö–æ" –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ–≤–∞.
            # –≠—Ç–æ –∏–º–∏—Ç–∏—Ä—É–µ—Ç —Å–ª—É—Ö–æ–≤—É—é –ø–µ—Ç–ª—é –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ (–º—ã —Å–ª—ã—à–∏–º, —á—Ç–æ –≥–æ–≤–æ—Ä–∏–º).
            echo_signal = self.interface.token_to_sensor_input(current_input_id)
            self.brain.body.sensory_input = echo_signal * 0.2
            
            # –î–∞–µ–º –º–æ–∑–≥—É –ø–æ–¥—É–º–∞—Ç—å (Generative process)
            for _ in range(steps_per_token):
                self.brain.step(self.dt)
            
            # –ß–∏—Ç–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ V1 (mu) - —á–µ–≥–æ –º–æ–∑–≥ –æ–∂–∏–¥–∞–µ—Ç —É—Å–ª—ã—à–∞—Ç—å –¥–∞–ª—å—à–µ?
            v1_prediction = self.brain.hierarchy.levels[0].mu
            
            # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –≤ —Å–ª–æ–≤–∞
            logits = self.interface.neural_state_to_logits(v1_prediction)
            
            # === REPETITION PENALTY (–ù–∞–∫–∞–∑–∞–Ω–∏–µ –∑–∞ –ø–æ–≤—Ç–æ—Ä—ã) ===
            # –í—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ —É–∂–µ —Å–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
            for past_token in history_window[-3:]: # –°–º–æ—Ç—Ä–∏–º –Ω–∞ 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ–≤–∞
                logits[past_token] -= 5.0 # –°–∏–ª—å–Ω–æ –ø–æ–Ω–∏–∂–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            
            # –ë–ª–æ–∫–∏—Ä—É–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–æ–∫–µ–Ω—ã –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            for bad_token in ["<UNK>", "<PAD>", "<BOS>"]:
                logits[self.tokenizer.token2id[bad_token]] = -float('inf')
            
            # –°—ç–º–ø–ª–∏—Ä—É–µ–º
            probs = torch.softmax(logits / temperature, dim=0)
            
            # –ó–∞—â–∏—Ç–∞ –æ—Ç NaN (–µ—Å–ª–∏ –≤–µ—Å–∞ —Å–æ–≤—Å–µ–º —É–ø–∞–ª–∏)
            if torch.isnan(probs).any():
                probs = torch.ones_like(probs) / len(probs)
            
            next_token_id = torch.multinomial(probs, 1).item()
            
            if next_token_id == self.tokenizer.token2id["<EOS>"]:
                break
                
            generated_ids.append(next_token_id)
            current_input_id = next_token_id
            history_window.append(next_token_id)

        return self.tokenizer.decode(generated_ids)

    def save_brain(self, filename="brain_dump.pt"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ (–í–µ—Å–∞ + –•–∏–º–∏—è + –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä)"""
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ '{filename}'...")
        
        # 1. –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å–∞ –∏–µ—Ä–∞—Ä—Ö–∏–∏
        hierarchy_state = []
        for level in self.brain.hierarchy.levels:
            # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–µ—Å–æ–≤
            def get_w(syn):
                if syn is None: return None
                if hasattr(syn, 'W_dense'): return syn.W_dense.cpu()
                if hasattr(syn, 'W'): return syn.W.cpu()
                return None

            level_data = {
                'W_bu': get_w(level.synapse_bottom_up),
                'W_td': get_w(level.synapse_top_down)
            }
            hierarchy_state.append(level_data)
            
        # 2. –°–æ–±–∏—Ä–∞–µ–º —Ö–∏–º–∏—é
        chemistry_state = {
            'dopamine': self.brain.chemistry.dopamine.cpu(),
            'norepinephrine': self.brain.chemistry.norepinephrine.cpu()
        }
        
        # 3. –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞—Ä—å (Interface)
        # –ù–∞–º –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–∞–º —Å–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤
        interface_state = {
            'embeddings': self.interface.embeddings.state_dict(),
            'projection': self.interface.W_sensor_projection.cpu(),
            'token2id': self.tokenizer.token2id,
            'id2token': self.tokenizer.id2token,
            'vocab_size': self.tokenizer.vocab_size
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª
        torch.save({
            'hierarchy': hierarchy_state,
            'chemistry': chemistry_state,
            'interface': interface_state
        }, filename)
        print("‚úÖ –ü–∞–º—è—Ç—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

    def load_brain(self, filename="brain_dump.pt"):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞"""
        import os
        if not os.path.exists(filename):
            print(f"‚ö†Ô∏è –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è '{filename}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è.")
            return

        print(f"\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞–º—è—Ç–∏ –∏–∑ '{filename}'...")
        checkpoint = torch.load(filename, map_location=self.interface.device)
        
        # 1. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
        saved_interface = checkpoint['interface']
        self.tokenizer.token2id = saved_interface['token2id']
        self.tokenizer.id2token = saved_interface['id2token']
        self.tokenizer.vocab_size = saved_interface['vocab_size']
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —Å–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–æ–¥ –Ω–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä
        self.interface.emb_dim = saved_interface['embeddings']['weight'].shape[1]
        self.interface.embeddings = torch.nn.Embedding(self.tokenizer.vocab_size, self.interface.emb_dim).to(self.interface.device)
        self.interface.embeddings.load_state_dict(saved_interface['embeddings'])
        self.interface.W_sensor_projection = saved_interface['projection'].to(self.interface.device)
        
        # 2. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–µ—Å–∞ –∏–µ—Ä–∞—Ä—Ö–∏–∏
        for i, level_data in enumerate(checkpoint['hierarchy']):
            if i >= len(self.brain.hierarchy.levels): break
            
            level = self.brain.hierarchy.levels[i]
            
            # Helper function for safe loading
            def load_w(syn, data):
                if syn is None or data is None: return
                
                device = self.interface.device
                if hasattr(syn, 'W_dense'):
                    syn.W_dense.data = data.to(device)
                    # Sync sparse if needed
                    if getattr(syn, 'is_sparse', False):
                        syn.W_sparse = syn.W_dense.to_sparse_csr()
                elif hasattr(syn, 'W'):
                    syn.W.data = data.to(device)

            # Bottom-Up
            load_w(level.synapse_bottom_up, level_data.get('W_bu'))
            
            # Top-Down
            load_w(level.synapse_top_down, level_data.get('W_td'))

        # 3. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ö–∏–º–∏—é
        self.brain.chemistry.dopamine.data = checkpoint['chemistry']['dopamine'].to(self.interface.device)
        self.brain.chemistry.norepinephrine.data = checkpoint['chemistry']['norepinephrine'].to(self.interface.device)
        
        print(f"‚úÖ –ê–≥–µ–Ω—Ç '–≤—Å–ø–æ–º–Ω–∏–ª' –ø—Ä–æ—à–ª—É—é –∂–∏–∑–Ω—å. Vocab: {self.tokenizer.vocab_size}")

# ==========================================
# 4. –ó–ê–ü–£–°–ö –î–ï–ú–û
# ==========================================

def run_education_session():
    # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞
    agent = TextAgent()
    
    # –ö–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    dataset = [
        "–Ø –º—ã—Å–ª—é —Å–æ–∑–Ω–∞–Ω–∏–µ",
        "–ë–æ–ª—å –µ—Å—Ç—å —Å–∏–≥–Ω–∞–ª",
        "–†–∞–¥–æ—Å—Ç—å —Å–≤–µ—Ç –∂–∏–∑–Ω—å",
        "–Ø —á—É–≤—Å—Ç–≤—É—é –≤—Ä–µ–º—è",
        "–ú—ã –≤–∏–¥–∏–º –º–∏—Ä",
        "–°–æ–∑–Ω–∞–Ω–∏–µ –µ—Å—Ç—å –ø–æ–∏—Å–∫",
        "–°–≤–µ—Ç –∏ —Ç—å–º–∞",
        "–Ø –µ—Å—Ç—å —Å—É–±—ä–µ–∫—Ç"
    ]
    
    start_time = time.time()
    
    # –û–±—É—á–µ–Ω–∏–µ
    # –ò–ó–ú–ï–ù–ï–ù–ò–ï 1: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
    # –î–∞–¥–∏–º –µ–º—É "–∑—É–±—Ä–∏—Ç—å" –º–∞—Ç–µ—Ä–∏–∞–ª –ø–æ–¥–æ–ª—å—à–µ.
    print("–ù–∞—á–∏–Ω–∞–µ–º —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
    for i, phrase in enumerate(dataset):
        agent.listen_and_learn(phrase, epochs=6) # –ë—ã–ª–æ 2, —Å—Ç–∞–≤–∏–º 6
        
    print(f"\n–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {time.time() - start_time:.2f} —Å–µ–∫.")
    
    # –¢–µ—Å—Ç
    print("\n=== –¢–ï–°–¢ –ì–ï–ù–ï–†–ê–¶–ò–ò (–î–∏–∞–ª–æ–≥) ===")
    prompts = ["–Ø", "–°–æ–∑–Ω–∞–Ω–∏–µ", "–ë–æ–ª—å", "–°–≤–µ—Ç"]
    
    # –ò–ó–ú–ï–ù–ï–ù–ò–ï 2: –°–Ω–∏–∂–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    # –≠—Ç–æ —Å–¥–µ–ª–∞–µ—Ç –µ–≥–æ –±–æ–ª–µ–µ "—Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω–Ω—ã–º" –∏ —É–±–µ—Ä–µ—Ç –±—Ä–µ–¥ —Ç–∏–ø–∞ "—Å–∏–Ω–∏–π".
    for p in prompts:
        # Temperature 0.5 (–±—ã–ª–æ 0.7-0.8). 
        # –ß–µ–º –Ω–∏–∂–µ, —Ç–µ–º —Å—Ç—Ä–æ–∂–µ –ª–æ–≥–∏–∫–∞.
        response = agent.generate_text(p, temperature=0.5)
        print(f"User: {p}")
        print(f"Agent: {response}")

if __name__ == "__main__":
    run_education_session()